---
title: "Week 4 Sum"
author: "Conner Langnas"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Jan 31

::: {.callout-important}
## TIL

Today we learned about 

1. Regression and linear regression
  * Motivation
  * 1_2 estimator
  * Inference
  * Prediction
2. Intro to statisical learning 
3. Multiple regression


#```{R results = 'hide'}
#| output: false
library(ISLR2)
library(dplyr)
#library(cowplot)
library(kableExtra)
library(htmlwidgets)
library(tidyverse)
```


# Statiscal Learning Diffrent types

* Supervised learning
  * Regression
  * Classification
* Unsupervised Learning
* Semi-Supervised Learning
* Renforcement learning




looking at scatter plots
using this data set 
https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt

#```{r}
url <- "https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt"
df <- read_tsv(url)
df %>% head(., 20) %>% kable
```

#```{r}
colnames(df) <- tolower(colnames(df))
x <- df$povpct
y <- df$brth15to17
```

#```{r}
plt <- function(){
  plot(
  x,
  y,
  pch=20,
  xlab = "Pov %"
  ylab = "Birth rate (15-17)"
)
}
plt()


```
#```{r}
b0 <- 1
b1 <- 2
plt()
curve(b0 + b1 *x, 0, 30, add=T, col='firebrick')
```
#```{r}
b0 <- c(-2, 0, 2)
b1 <- c(0, 1, 2)
par(mfrow=c(3,3))
for(B0 in b0){
  for(B1 in b1){
    plt()
    curve(B0 + B1 *x, 0, 30, add=T, col='firebrick')
    title(main = paste('b0 = ', B0, ' and b1 = ', B1))
  }
}
```
goal is get a straight line that fits most of the data


#```{r, fig.height=9, fig.width=12}
b0 <- 10
b1 <- 1.1
yhat <- b0 + b1 * x
plt()
curve(b0 + b1 * x, 0, 30, add=T, col='firebrick')
title(main = paste('b0 = ', b0, ' and b1 = ', b1))
segments(x, y, x, yhat)
resids <- abs(y - yhat)^2
ss_resids <- sum(resids)
title(main = paste('b0, b1, ss_residuals = ', b0, b1, ss_resids, sep=','))
```






## Thursday, Feb 2

Today, we learned 

1. Linear Regression
1. Multiple Regression


In our case we want to model $y$ as a function of $x$. In 'R' the formula for this looks like:

#```{r}
typeof(formula(y ~ x))
```
Linear regression model in `R` is called using the **L**inear **M**odel, i.e., 'lm()'

#```{r}
model <- lm(y ~ x)
```

#```{r}
summary(model)
```

```{r}
x <- seq(0, 5, length=100)
b0 <- 1
b1 <- 3
y1 <- b0 + b1 * x + rnorm(100)
y2 <- b0 + b1 * x + rnorm(100) * 3
par(mfrow=c(1,2))
plot(x, y1)
plot(x, y2)
```

```{r}
model1 <- lm(y1 ~ x)
model2 <- lm(y2 ~ x)
par(mfrow=c(1,2))
plot(x, y1)
curve(
  coef(model1)[1] + coef(model1)[2] * x,
  add=T, col="red"
)
plot(x, y2)
curve(
  coef(model2)[1] + coef(model2)[2] * x,
  add=T, col="red"
)
```


```{r}
summary(model1)
```


```{r}
summary(model2)
```


now to predict models


#```{r}
x <- df$povpct
y <- df$brth15to17
plt()
```

Suppose we have a "new" state formed whose 'povpct' value is $22$.

#```{r}
plt()
abline(v=21, col="green")
lines(x, fitted(lm(y~x)), col="red")
```

**Q.** What is the best guess for this prediction going to be? We could consider the graph and our best prediction is going to be the intersection. In $R$, we can use the `predict()` function to do this:

#```{r}
new_x <- data.frame(x = c(21))
new_y <- predict(model, new_x)
new_y
```

#```{r}
plt()
abline(v=21, col="green")
lines(x, fitted(lm(y~x)), col="red")
points(new_x, new_y, col="purple")
```

We can make predictions not just for a single observation, but for a whole collection of observations.

#```{r}
new_x <- data.frame(x = c(1:21))
new_y <- predict(model, new_x)
```

This is what the plot looks like:

#```{r}
plt()
for(a in new_x){abline(v=a, col="green")}
lines(x, fitted(lm(y~x)), col="red")
points(a, new_y, col="purple")
```