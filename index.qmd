---
title: "Week 4 Sum"
author: "Conner Langnas"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Jan 31

::: {.callout-important}
## TIL

Today we learned about 

1. Regression and linear regression
  * Motivation
  * 1_2 estimator
  * Inference
  * Prediction
2. Intro to statisical learning 
3. Multiple regression


```{R results = 'hide'}
#| output: false
library(ISLR2)
library(dplyr)
#library(cowplot)
library(kableExtra)
library(htmlwidgets)
library(tidyverse)
```


## Statiscal Learning Diffrent types

* Supervised learning
  * Regression
  * Classification
* Unsupervised Learning
* Semi-Supervised Learning
* Renforcement learning




looking at scatter plots
using this data set 
https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt

```{r}
url <- "https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt"
df <- read_tsv(url)
df %>% head(., 20) %>% kable
```

```{r}
colnames(df) <- tolower(colnames(df))
x <- df$povpct
y <- df$brth15to17
```

```{r}
plt <- function(){
  plot(
  x,
  y,
  pch=20,
  xlab = "Pov %"
  ylab = "Birth rate (15-17)"
)
}
plt()


```
```{r}
b0 <- 1
b1 <- 2
plt()
curve(b0 + b1 *x, 0, 30, add=T, col='firebrick')
```
```{r}
b0 <- c(-2, 0, 2)
b1 <- c(0, 1, 2)
par(mfrow=c(3,3))
for(B0 in b0){
  for(B1 in b1){
    plt()
    curve(B0 + B1 *x, 0, 30, add=T, col='firebrick')
    title(main = paste('b0 = ', B0, ' and b1 = ', B1))
  }
}
```
goal is get a straight line that fits most of the data


```{r, fig.height=9, fig.width=12}
b0 <- 10
b1 <- 1.1
yhat <- b0 + b1 * x
plt()
curve(b0 + b1 * x, 0, 30, add=T, col='firebrick')
title(main = paste('b0 = ', b0, ' and b1 = ', b1))
segments(x, y, x, yhat)
resids <- abs(y - yhat)^2
ss_resids <- sum(resids)
title(main = paste('b0, b1, ss_residuals = ', b0, b1, ss_resids, sep=','))
```






## Thursday, Feb 2

Today, we learned 

1. Linear Regression
1. 

